{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "09_text_classification",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEaxnPDUycylSVRO0rjmHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Tiger/ml-project/blob/main/09_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBtBKl2Qb12Z"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtFDlC_zb-60",
        "outputId": "aea2b3ee-ebd7-4f40-a12f-cabcb272c008"
      },
      "source": [
        "!pip install janome beautifulsoup4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 21.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Installing collected packages: janome\n",
            "Successfully installed janome-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK-N0XrmcEKw",
        "outputId": "b7112193-3cb9-4869-a5dd-f22635c0e811"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir models\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz -P data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-10 08:45:51--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1279641604 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘data/cc.ja.300.vec.gz’\n",
            "\n",
            "cc.ja.300.vec.gz    100%[===================>]   1.19G  15.2MB/s    in 74s     \n",
            "\n",
            "2021-05-10 08:47:05 (16.5 MB/s) - ‘data/cc.ja.300.vec.gz’ saved [1279641604/1279641604]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixCpiTzHg5GJ"
      },
      "source": [
        "import string\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "from janome.tokenizer import Tokenizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding, SimpleRNN, LSTM, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucP3pk9CcOFB"
      },
      "source": [
        "maxlen = 300\n",
        "num_words = 40000\n",
        "num_label = 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQynPwocoEB"
      },
      "source": [
        "def filter_by_ascii_rate(text, threshold=0.9):\n",
        "    ascii_letters = set(string.printable)\n",
        "    rate = sum(c in ascii_letters for c in text) / len(text)\n",
        "    return rate <= threshold\n",
        "\n",
        "def load_dataset(filename, n=5000, state=6):\n",
        "    df = pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "    # Converts multi-class to binary-class.\n",
        "    mapping = {1: 0, 2: 0, 4: 1, 5: 1}\n",
        "    df = df[df.star_rating !=3]\n",
        "    df.star_rating = df.star_rating.map(mapping)\n",
        "\n",
        "    # extracts Japanese texts:\n",
        "    is_jp = df.review_body.apply(filter_by_ascii_rate)\n",
        "    df = df[is_jp]\n",
        "\n",
        "    # sampling.\n",
        "    df = df.sample(frac=1, random_state=state)\n",
        "    grouped = df.groupby('star_rating')\n",
        "    df = grouped.head(n=n)\n",
        "    return df.review_body.values, df.star_rating.values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1h0219HggjS"
      },
      "source": [
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz'\n",
        "x, y = load_dataset(url)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvKzW11jgxHs"
      },
      "source": [
        "def load_fasttext(filepath, binary=False):\n",
        "    \"\"\"Loads fastText vectors.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): a path to a fastText file.\n",
        "\n",
        "    Return:\n",
        "        model: KeyedVectors\n",
        "    \"\"\"\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=binary)\n",
        "    return model\n",
        "\n",
        "\n",
        "wv = load_fasttext('/content/data/cc.ja.300.vec.gz')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZbnIOM89iUF"
      },
      "source": [
        "t = Tokenizer(wakati=True)\n",
        "\n",
        "\n",
        "def build_vocabulary(texts, num_words=None):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=num_words, oov_token='<UNK>'\n",
        "    )\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def clean_html(html, strip=False):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    text = soup.get_text(strip=strip)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return t.tokenize(text)\n",
        "\n",
        "\n",
        "def preprocess_dataset(texts):\n",
        "    texts = [clean_html(text) for text in texts]\n",
        "    texts = [' '.join(tokenize(text)) for text in texts]\n",
        "    return texts\n",
        "\n",
        "\n",
        "def filter_embeddings(embeddings, vocab, num_words, dim=300):\n",
        "  \"\"\"Filter word vectors.\n",
        "\n",
        "  Args:\n",
        "      embeddings: a dictionary like object.\n",
        "      vocab: word-index lookup table.\n",
        "      num_words: the number of words.\n",
        "      dim: dimension.\n",
        "\n",
        "  Returns:\n",
        "      numpy array: an array of word embeddings.\n",
        "  \"\"\"\n",
        "  _embeddings = np.zeros((num_words, dim))\n",
        "  for word in vocab:\n",
        "      if word in embeddings:\n",
        "          word_id = vocab[word]\n",
        "          if word_id >= num_words:\n",
        "              continue\n",
        "          _embeddings[word_id] = embeddings[word]\n",
        "\n",
        "  return _embeddings"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB3fDIxJ_KjR"
      },
      "source": [
        "x = preprocess_dataset(x)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "vocab = build_vocabulary(x_train, num_words)\n",
        "x_train = vocab.texts_to_sequences(x_train)\n",
        "x_test = vocab.texts_to_sequences(x_test)\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen, truncating='post', padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen, truncating='post', padding='post')\n",
        "\n",
        "wv = filter_embeddings(wv, vocab.word_index, num_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08vvL-RU_Pa8"
      },
      "source": [
        "class RNNModel:\n",
        "\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 emb_dim=300, hid_dim=100,\n",
        "                 embeddings=None, trainable=True):\n",
        "        self.input = Input(shape=(None,), name='input')\n",
        "        if embeddings is None:\n",
        "            self.embedding = Embedding(input_dim=input_dim,\n",
        "                                       output_dim=emb_dim,\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=trainable,\n",
        "                                       name='embedding')\n",
        "            \n",
        "        else:\n",
        "            self.embedding = Embedding(input_dim=embeddings.shape[0],\n",
        "                                       output_dim=embeddings.shape[1],\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=trainable,\n",
        "                                       weights=[embeddings],\n",
        "                                       name='embedding')\n",
        "        self.rnn = SimpleRNN(hid_dim, name='rnn')\n",
        "        self.fc = Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def build(self):\n",
        "        x = self.input\n",
        "        embedding = self.embedding(x)\n",
        "        output = self.rnn(embedding)\n",
        "        y = self.fc(output)\n",
        "        return Model(inputs=x, outputs=y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhsEuK-ESK-"
      },
      "source": [
        "class LSTMModel:\n",
        "\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 emb_dim=300, hid_dim=100,\n",
        "                 embeddings=None, trainable=True):\n",
        "        self.input = Input(shape=(None,), name='input')\n",
        "        if embedding is None:\n",
        "            self.embedding = Embedding(input_dim=input_dim,\n",
        "                                       output_dim=emb_dim,\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=trainable,\n",
        "                                       name='embedding')\n",
        "            \n",
        "        else:\n",
        "            self.embedding = Embedding(input_dim=embeddings.shape[0],\n",
        "                                       output_dim=embeddings.shape[1],\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=trainable,\n",
        "                                       weights=[embeddings],\n",
        "                                       name='embedding')\n",
        "        self.lstm = LSTM(hid_dim, name='lstm')\n",
        "        self.fc = Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def build(self):\n",
        "        x = self.input\n",
        "        embedding = self.embedding(x)\n",
        "        output = self.lstm(embedding)\n",
        "        y = self.fc(output)\n",
        "        return Model(inputs=x, outputs=y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNRERMIEGwLL"
      },
      "source": [
        "class CNNModel:\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 filters=250, kernel_size=3,\n",
        "                 emb_dim=300, embeddings=None, trainable=True):\n",
        "        self.input = Input(shape=(None,), name='input')\n",
        "        if embeddings is None:\n",
        "            self.embedding = Embedding(input_dim=input_dim,\n",
        "                                       output_dim=emb_dim,\n",
        "                                       trainable=trainable,\n",
        "                                       name='embedding')\n",
        "            \n",
        "        else:\n",
        "            self.embedding = Embedding(input_dim=embeddings.shape[0],\n",
        "                                       output_dim=embeddings.shape[1],\n",
        "                                       trainable=trainable,\n",
        "                                       weights=[embeddings],\n",
        "                                       name='embedding')\n",
        "        \n",
        "        self.conv = Conv1D(filters,\n",
        "                           kernel_size,\n",
        "                           padding='valid',\n",
        "                           activation='relu',\n",
        "                           strides=1)\n",
        "        self.pool = GlobalMaxPooling1D()\n",
        "        self.fc = Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def build(self):\n",
        "        x = self.input\n",
        "        embedding = self.embedding(x)\n",
        "        conv = self.conv(embedding)\n",
        "        pool = self.pool(conv)\n",
        "        y = self.fc(pool)\n",
        "        return Model(inputs=x, outputs=y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8RCy0eK3K7"
      },
      "source": [
        "models = [\n",
        "          RNNModel,\n",
        "          LSTMModel,\n",
        "          CNNModel,\n",
        "          CNNModel\n",
        "]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQzHbTatLKXA",
        "outputId": "f820c8ef-e08b-42cf-a03d-6ad1c2ba2b52"
      },
      "source": [
        "model_path = 'models/model_{}'\n",
        "embeddings = [None, None, None, wv]\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "i = 0\n",
        "for model, embedding in zip(models, embeddings):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = model(num_words, num_label, embeddings=embedding).build()\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=3),\n",
        "        ModelCheckpoint(model_path.format(i), save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        x=x_train, y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=callbacks,\n",
        "        shuffle=True\n",
        "    )\n",
        "    i += 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - 25s 422ms/step - loss: 0.6888 - acc: 0.5435 - val_loss: 0.6410 - val_acc: 0.6488\n",
            "INFO:tensorflow:Assets written to: models/model_0/assets\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 21s 417ms/step - loss: 0.4595 - acc: 0.7960 - val_loss: 0.6201 - val_acc: 0.7081\n",
            "INFO:tensorflow:Assets written to: models/model_0/assets\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 20s 407ms/step - loss: 0.0881 - acc: 0.9745 - val_loss: 0.6776 - val_acc: 0.6944\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 21s 411ms/step - loss: 0.0120 - acc: 0.9994 - val_loss: 0.7254 - val_acc: 0.6969\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 20s 405ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7688 - val_acc: 0.6844\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 43s 277ms/step - loss: 0.6235 - acc: 0.6368 - val_loss: 0.4870 - val_acc: 0.7931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "50/50 [==============================] - 13s 263ms/step - loss: 0.2622 - acc: 0.9007 - val_loss: 0.4185 - val_acc: 0.8056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/100\n",
            "50/50 [==============================] - 13s 262ms/step - loss: 0.1279 - acc: 0.9606 - val_loss: 0.5526 - val_acc: 0.8075\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 13s 259ms/step - loss: 0.0534 - acc: 0.9852 - val_loss: 0.6388 - val_acc: 0.8044\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 13s 264ms/step - loss: 0.0183 - acc: 0.9967 - val_loss: 0.6226 - val_acc: 0.7669\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 13s 221ms/step - loss: 0.6604 - acc: 0.6099 - val_loss: 0.5382 - val_acc: 0.7469\n",
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "50/50 [==============================] - 10s 204ms/step - loss: 0.4421 - acc: 0.8221 - val_loss: 0.4188 - val_acc: 0.8206\n",
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/100\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.2406 - acc: 0.9287 - val_loss: 0.3875 - val_acc: 0.8306\n",
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_2/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/100\n",
            "50/50 [==============================] - 10s 208ms/step - loss: 0.1055 - acc: 0.9790 - val_loss: 0.3971 - val_acc: 0.8313\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 10s 206ms/step - loss: 0.0356 - acc: 0.9977 - val_loss: 0.4105 - val_acc: 0.8319\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 10s 202ms/step - loss: 0.0122 - acc: 0.9999 - val_loss: 0.4400 - val_acc: 0.8300\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 11s 209ms/step - loss: 0.6522 - acc: 0.6096 - val_loss: 0.4612 - val_acc: 0.7956\n",
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "50/50 [==============================] - 11s 214ms/step - loss: 0.3385 - acc: 0.8896 - val_loss: 0.3927 - val_acc: 0.8281\n",
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/100\n",
            "50/50 [==============================] - 10s 210ms/step - loss: 0.2028 - acc: 0.9497 - val_loss: 0.3746 - val_acc: 0.8406\n",
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/100\n",
            "50/50 [==============================] - 10s 208ms/step - loss: 0.1052 - acc: 0.9863 - val_loss: 0.3736 - val_acc: 0.8450\n",
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: models/model_3/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/100\n",
            "50/50 [==============================] - 10s 205ms/step - loss: 0.0507 - acc: 0.9988 - val_loss: 0.3878 - val_acc: 0.8381\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 11s 216ms/step - loss: 0.0250 - acc: 0.9995 - val_loss: 0.4090 - val_acc: 0.8363\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 11s 214ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.8369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUX91hXJLtXS"
      },
      "source": [
        "class InferenceAPI:\n",
        "    \"\"\"A model API that generates output sequence.\n",
        "\n",
        "    Attributes:\n",
        "        model: Model.\n",
        "        vocab: language's vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, vocal, preprocess):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def predict_from_texts(self, texts):\n",
        "        x = self.preprocess(texts)\n",
        "        x = self.vocab.texts_to_sequences(x)\n",
        "        return self.predict_from_sequences(x)\n",
        "\n",
        "    def predict_from_sequences(self, sequences):\n",
        "        sequences = pad_sequences(sequences, truncating='post')\n",
        "        y = self.model.predict(sequences)\n",
        "        return np.argmax(y, -1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upAxKREMQjpS",
        "outputId": "417bbe3e-2258-4144-bb79-bad04531a39a"
      },
      "source": [
        "model_names = ['RNN', 'LSTM', 'CNN', 'CNN(wv)']\n",
        "for i, model_name in enumerate(model_names):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = load_model(model_path.format(i))\n",
        "    api = InferenceAPI(model, vocab, preprocess_dataset)\n",
        "    y_pred = api.predict_from_sequences(x_test)\n",
        "    print(model_name)\n",
        "    print('precision\\t: {:.4f}'.format(precision_score(y_test, y_pred, average='binary')))\n",
        "    print('recall\\t: {:.4f}'.format(recall_score(y_test, y_pred, average='binary')))\n",
        "    print('f1\\t: {:.4f}'.format(f1_score(y_test, y_pred, average='binary')))\n",
        "    print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN\n",
            "precision\t: 0.7312\n",
            "recall\t: 0.7146\n",
            "f1\t: 0.7228\n",
            "\n",
            "LSTM\n",
            "precision\t: 0.8709\n",
            "recall\t: 0.7889\n",
            "f1\t: 0.8279\n",
            "\n",
            "CNN\n",
            "precision\t: 0.8919\n",
            "recall\t: 0.8097\n",
            "f1\t: 0.8488\n",
            "\n",
            "CNN(wv)\n",
            "precision\t: 0.8686\n",
            "recall\t: 0.8385\n",
            "f1\t: 0.8533\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}